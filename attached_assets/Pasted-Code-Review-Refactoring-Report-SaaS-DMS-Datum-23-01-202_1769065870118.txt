Code Review & Refactoring Report: SaaS-DMS
Datum: 23.01.2026 Status: üî¥ CRITICAL CHANGE REQUESTS Reviewer: Senior Lead Developer / Cyber Security Analyst Scope: Migration von "On-Premise Thinking" zu "Cloud-Native SaaS" (Azure)

1. Executive Summary
Der aktuelle Code-Stand stellt einen funktionalen Prototypen dar, ist jedoch nicht mandantenf√§hig (Multi-Tenant safe) und f√ºr den Betrieb in einer Azure-Container-Umgebung ungeeignet.

Kritische Architekturfehler liegen in der Verwendung lokaler Dateisystempfade (pathlib, Samba-Shares) und globaler Einstellungen (Singleton-Pattern), die eine saubere Datentrennung unm√∂glich machen. Die Sicherheitsarchitektur weist DoS-Vektoren in der Verschl√ºsselung und DSGVO-Verst√∂√üe bei der Datenl√∂schung auf.

Zielsetzung des Refactorings:

Vollst√§ndige Entfernung aller Samba- und Filesystem-Abh√§ngigkeiten.

Umstellung auf Azure Blob Storage und API-basierte Ingest-Wege.

H√§rtung der Mandanten-Isolierung und Kryptografie.

2. Kritische Sicherheitsl√ºcken & Architektur (Must-Fix)
A. Echte Mandantenf√§higkeit herstellen
Problem: dms/models.py -> SystemSettings ist ein Singleton (pk=1). Alle Mandanten teilen sich aktuell einen Sage-Account und einen E-Mail-Server.

L√∂sung:

Entfernen Sie sage_cloud_api_url, ms_graph_* Credentials aus SystemSettings.

Migrieren Sie diese Felder in das Tenant Modell. Jeder Mandant ben√∂tigt (optional) eigene Connectors oder das System nutzt eine mandantenf√§hige App-Registrierung, die Token pro Mandant trennt.

Fix: SystemLog muss zwingend ein Feld tenant erhalten. Aktuell leaken Logs global.

B. Kryptografie & DoS-Schutz
Problem: dms/encryption.py -> encrypt_file l√§dt ganze Dateien in den RAM.

Risiko: 10 gleichzeitige Uploads √° 100MB f√ºhren zum Container-Absturz (OOM Kill).

L√∂sung: Umstellung auf Streaming Encryption. Nutzen Sie Cipher aus cryptography.hazmat mit Chunks (z.B. 64KB), die direkt vom Network-Stream (Azure Blob / API Request) in den Storage-Stream flie√üen.

Key-Management: Abkehr vom globalen ENCRYPTION_KEY. Implementierung von Tenant-Keys (DEK), die mit dem Master-Key (KEK) verschl√ºsselt in der DB liegen.

C. DSGVO & Compliance
Problem: accept_invite speichert IP-Adressen im Klartext.

L√∂sung: IP-Adressen vor dem Speichern maskieren (letztes Oktett nullen) oder L√∂schkonzept implementieren.

Problem: SystemSettings enth√§lt unsicheren Key-Fallback (dev-only-insecure-key).

L√∂sung: In settings.py: Wenn DEBUG=False und kein Key vorhanden -> sys.exit(1). Kein Fallback in Production!

3. Architektur-Pivot: Azure Native (Neue Anforderungen)
A. Abl√∂sung Samba & Lokale Scans (Scanner .exe)
Die Architektur "Server √ºberwacht Ordner" ist in Azure Web Apps nicht m√∂glich.

Veraltete Artefakte l√∂schen:

dms/management/commands/generate_samba_config.py

scripts/map_shares.bat & .ps1

Entfernen Sie Samba-Felder aus SystemSettings in models.py.

Neuer Ingest-Weg (API):

Erstellen Sie einen dedizierten API-Endpunkt f√ºr die lokale .exe Software des Kunden.

Endpoint: POST /api/v1/ingest/document/

Auth: Header X-DMS-Token: <tenant.ingest_token>

Logik: Datei validieren -> Stream-Verschl√ºsselung -> Upload zu Azure Blob Storage -> Eintrag in DB (ProcessedFile).

Task-Anpassung: Der Task scan_manual_input wird obsolet, da der Server nicht mehr scannt, sondern passiv empf√§ngt.

B. Azure Blob Storage Integration (Sage Archiv)
Der "Sage Archive Scan" darf nicht mehr auf pathlib basieren.

Refactoring scan_sage_archive:

Statt path.iterdir() Nutzung des BlobServiceClient (Azure SDK).

Der Kunde (oder ein Job) l√§dt Sage-Daten in einen Azure Blob Container hoch.

Der Task iteriert √ºber Blobs (list_blobs), pr√ºft Metadaten/Pfad im Blob-Namen und streamt den Content zur Verarbeitung.

Wichtig: Kein Zwischenspeichern auf der lokalen Container-Disk!

C. MS Graph E-Mail Integration
Routing: E-Mails m√ºssen anhand des To-Headers (z.B. upload.<token>@dms.cloud) dem Tenant zugeordnet werden.

L√∂schkonzept:

Das aktuelle message.delete() ist ein Soft-Delete.

Requirement: E-Mails m√ºssen nach Verarbeitung unwiderruflich weg sein.

Umsetzung: Kombination aus message.delete() und einer Exchange Online Retention Policy (1 Tag retention), da API Hard-Deletes komplex und fehleranf√§llig sind.

Reply-Routing: Beim Versand von E-Mails aus dem System muss der Reply-To Header zwingend auf die systemspezifische Ingest-Adresse des Mandanten gesetzt werden.

4. Technischer Action-Plan (Checklist f√ºr Entwickler)
Phase 1: Cleanup & Security (Sofort)
[ ] L√∂schen: Alle Samba-Skripte und generate_samba_config.py.

[ ] Modell-Update: SystemSettings bereinigen (kein Samba PW, kein globales Sage Login).

[ ] Modell-Update: Tenant Modell um API-Config Felder erweitern (sofern Mandanten eigene Connectors brauchen).

[ ] Security: encrypt_file und decrypt_data auf Streaming-Verarbeitung umschreiben (Chunk-Gr√∂√üe max 64KB).

[ ] Security: settings.py -> SECRET_KEY Logik h√§rten (Crash statt Default-Key in Prod).

Phase 2: API & Storage (Architektur)
[ ] API View: Implementierung api_upload_document (Stateless, Token Auth) in views.py.

[ ] Task Refactoring: scan_sage_archive neu schreiben -> Nutzung von azure-storage-blob statt os.path / pathlib.

[ ] Task Cleanup: scan_manual_input entfernen (ersetzt durch API View).

[ ] Graph Integration: poll_central_inbox_graph pr√ºfen: Fehlerbehandlung bei fehlendem Token verbessern (Quarant√§ne-Logik ist gut, beibehalten).

Phase 3: Infrastruktur & Deployment
[ ] Docker: Entfernen der Mounts /srv/sage_archiv und /srv/manual_scan aus docker-compose.yml.

[ ] Environment: Neue Env-Vars f√ºr Azure Storage Connection Strings definieren.

Code-Beispiel: API Upload View (Ersatz f√ºr Folder Watch)
Python
# In dms/views.py oder neuem dms/api.py
from django.views.decorators.csrf import csrf_exempt
from django.http import JsonResponse
from .models import Tenant, Document
from .encryption import encrypt_data_streaming # Neu zu implementieren

@csrf_exempt
@require_http_methods(["POST"])
def api_upload_ingest(request):
    # 1. Auth via Token Header (f√ºr .exe Client)
    token = request.headers.get('X-DMS-Token')
    if not token:
        return JsonResponse({'error': 'Missing Token'}, status=401)
    
    try:
        tenant = Tenant.objects.get(ingest_token=token, is_active=True)
    except Tenant.DoesNotExist:
        return JsonResponse({'error': 'Invalid Token'}, status=403)

    # 2. Datei Verarbeitung (Streaming!)
    if 'file' not in request.FILES:
        return JsonResponse({'error': 'No file'}, status=400)
        
    file_obj = request.FILES['file']
    
    # Hier Streaming-Encryption aufrufen und direkt nach Azure Blob pipen
    # ...
    
    return JsonResponse({'status': 'success', 'document_id': str(new_doc.id)})